{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95607582",
   "metadata": {},
   "source": [
    "# LLRNet: Model training and testing\n",
    "The wireless ML design flow using Aerial is depicted in the figure below. \n",
    "\n",
    "![ML design flow](ml_design_flow.png \"ML design flow\")\n",
    "\n",
    "\n",
    "In this notebook, we use the generated LLRNet data for training and validating LLRNet as part of the PUSCH receiver chain, implemented using pyAerial, with Aerial SDK cuPHY library working as the backend. The LLRNet is plugged in the PUSCH receiver chain in place of the conventional soft demapper. So this notebook works as an example of using pyAerial for model validation.\n",
    "\n",
    "Finally, the model is exported into a format consumed by the TensorRT inference engine that is used for integrating the model into Aerial SDK for testing the model with real hardware in an over the air environment.\n",
    "\n",
    "**Note 1:** This notebook requires that the Aerial test vectors have been generated. The test vector directory is set below in `AERIAL_TEST_VECTOR_DIR` variable.\n",
    "**Note 2:** This notebook also requires that the notebook example on LLRNet dataset generation has been run first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545036b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"  # Silence TensorFlow.\n",
    "os.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\"\n",
    "\n",
    "import cuda\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tf2onnx\n",
    "import onnx\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "# PyAerial components\n",
    "from aerial.phy5g.algorithms import ChannelEstimator\n",
    "from aerial.phy5g.algorithms import ChannelEqualizer\n",
    "from aerial.phy5g.algorithms import NoiseIntfEstimator\n",
    "from aerial.phy5g.algorithms import Demapper\n",
    "from aerial.phy5g.algorithms import TrtEngine\n",
    "from aerial.phy5g.algorithms import TrtTensorPrms\n",
    "from aerial.phy5g.ldpc import LdpcDeRateMatch\n",
    "from aerial.phy5g.ldpc import LdpcDecoder\n",
    "from aerial.phy5g.ldpc import CrcChecker\n",
    "from aerial.phy5g.config import PuschConfig\n",
    "from aerial.phy5g.config import PuschUeConfig\n",
    "from aerial.util.cuda import get_cuda_stream\n",
    "from aerial.util.data import load_pickle\n",
    "from aerial.util.fapi import dmrs_fapi_to_bit_array\n",
    "\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed.\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_errors = dict(aerial=dict(), llrnet=dict(), logmap=dict())\n",
    "tb_count = dict(aerial=dict(), llrnet=dict(), logmap=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset root directory.\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Aerial test vector directory.\n",
    "AERIAL_TEST_VECTOR_DIR = \"/mnt/cicd_tvs/develop/GPU_test_input/\"\n",
    "\n",
    "# LLRNet dataset directory.\n",
    "dataset_dir = DATA_DIR + \"example_llrnet_dataset/QPSK/\"\n",
    "\n",
    "# LLRNet model target path\n",
    "llrnet_onnx_file = f\"../models/llrnet.onnx\"\n",
    "llrnet_trt_file = f\"../models/llrnet.trt\"\n",
    "\n",
    "# Training vs. testing SNR. Assume these exist in the dataset.\n",
    "train_snr = [-7.75, -7.5, -7.25, -7.0, -6.75, -6.5]\n",
    "test_snr = [-7.75, -7.5, -7.25, -7.0, -6.75, -6.5]\n",
    "\n",
    "# Training, validation and test split in percentages if the same SNR is used for\n",
    "# training and testing.\n",
    "train_split = 45\n",
    "val_split = 5\n",
    "test_split = 50\n",
    "\n",
    "# Training hyperparameters.\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "step = tf.Variable(0, trainable=False)\n",
    "boundaries = [350000, 450000]\n",
    "values = [5e-4, 1e-4, 1e-5]\n",
    "# values = [0.05, 0.01, 0.001]\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn, weight_decay=1e-4)\n",
    "# optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.05, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "# Modulation order. LLRNet needs to be trained separately for each modulation order.\n",
    "mod_order = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ce6a4",
   "metadata": {},
   "source": [
    "## Define the LLRNet model\n",
    "The LLRNet model follows the original paper\n",
    "\n",
    "O. Shental, J. Hoydis, \"*'Machine LLRning': Learning to Softly Demodulate*\", https://arxiv.org/abs/1907.01512\n",
    "\n",
    "and is a very simple MLP model with a single hidden layer. It takes the equalized symbols in its input with the real and imaginary parts separated, and outputs soft bits (log-likelihood ratios) that can be further fed into LDPC (de)rate matching and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(2,))\n",
    "x = keras.layers.Dense(16, activation=\"relu\")(inputs)\n",
    "outputs = keras.layers.Dense(8, activation=\"linear\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def loss(llr, predictions):\n",
    "    mae = tf.abs(predictions[:, :mod_order] - llr)\n",
    "    mse = tf.reduce_mean(tf.square(mae))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef2251",
   "metadata": {},
   "source": [
    "## Training, validation and testing datasets\n",
    "Here, the dataset gets loaded and spåålit into training, validation and testing datasets, as well as put in the right format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main data file\n",
    "try:\n",
    "    df = pd.read_parquet(dataset_dir + \"l2_metadata.parquet\", engine=\"pyarrow\")\n",
    "except FileNotFoundError:\n",
    "    display(Markdown(\"**Data not found - has llrnet_dataset_generation.ipynb been run?**\"))\n",
    "\n",
    "# Query the entries for the selected modulation order.\n",
    "df = df[df[\"qamModOrder\"] == mod_order]\n",
    "\n",
    "# Collect the dataset by SNR.\n",
    "llrs = dict()\n",
    "eq_syms = dict()\n",
    "indices = dict()\n",
    "for pusch_record in df.itertuples():\n",
    "    user_data_filename = dataset_dir + pusch_record.user_data_filename\n",
    "    user_data = load_pickle(user_data_filename)\n",
    "\n",
    "    if user_data[\"snr\"] not in llrs.keys():\n",
    "        llrs[user_data[\"snr\"]] = []\n",
    "        eq_syms[user_data[\"snr\"]] = []\n",
    "        indices[user_data[\"snr\"]] = []\n",
    "\n",
    "    llrs[user_data[\"snr\"]].append(user_data[\"map_llrs\"])\n",
    "    eq_syms[user_data[\"snr\"]].append(user_data[\"eq_syms\"])\n",
    "\n",
    "    indices[user_data[\"snr\"]].append(pusch_record.Index)\n",
    "\n",
    "llr_train, llr_val = [], []\n",
    "sym_train, sym_val = [], []\n",
    "test_indices = []\n",
    "for key in llrs.keys():\n",
    "    llrs[key] = np.stack(llrs[key])\n",
    "    eq_syms[key] = np.stack(eq_syms[key])\n",
    "\n",
    "    # Randomize the order.\n",
    "    permutation = np.arange(llrs[key].shape[0])\n",
    "    np.random.shuffle(permutation)\n",
    "    llrs[key] = llrs[key][permutation, ...]\n",
    "    eq_syms[key] = eq_syms[key][permutation, ...]\n",
    "    indices[key] = list(np.array(indices[key])[permutation])\n",
    "\n",
    "    # Separate real and imaginary parts of the symbols.\n",
    "    eq_syms[key] = np.stack((np.real(eq_syms[key]), np.imag(eq_syms[key])))\n",
    "\n",
    "    num_slots = llrs[key].shape[0]\n",
    "    if key in train_snr and key in test_snr:\n",
    "        num_train_slots = int(np.round(train_split / 100 * num_slots))\n",
    "        num_val_slots = int(np.round(val_split / 100 * num_slots))\n",
    "        num_test_slots = int(np.round(test_split / 100 * num_slots))\n",
    "    elif key in train_snr:\n",
    "        num_train_slots = int(np.round(train_split / (train_split + val_split) * num_slots))\n",
    "        num_val_slots = int(np.round(val_split / (train_split + val_split) * num_slots))\n",
    "        num_test_slots = 0\n",
    "    elif key in test_snr:\n",
    "        num_train_slots = 0\n",
    "        num_val_slots = 0\n",
    "        num_test_slots = num_slots\n",
    "    else:\n",
    "        num_train_slots = 0\n",
    "        num_val_slots = 0\n",
    "        num_test_slots = 0\n",
    "\n",
    "    # Collect training/validation/testing sets.\n",
    "    llr_train.append(llrs[key][:num_train_slots, ...])\n",
    "    llr_val.append(llrs[key][num_train_slots:num_train_slots+num_val_slots, ...])\n",
    "    sym_train.append(eq_syms[key][:, :num_train_slots, ...])\n",
    "    sym_val.append(eq_syms[key][:, num_train_slots:num_train_slots+num_val_slots, ...])\n",
    "    # Just indices for the test set.\n",
    "    test_indices += indices[key][num_train_slots+num_val_slots:num_train_slots+num_val_slots+num_test_slots]\n",
    "\n",
    "llr_train = np.transpose(np.concatenate(llr_train, axis=0), (1, 0, 2))\n",
    "llr_val = np.transpose(np.concatenate(llr_val, axis=0), (1, 0, 2))\n",
    "sym_train = np.concatenate(sym_train, axis=1)\n",
    "sym_val = np.concatenate(sym_val, axis=1)\n",
    "\n",
    "# Fetch the total number of slots in each set.\n",
    "num_train_slots = llr_train.shape[1]\n",
    "num_val_slots = llr_val.shape[1]\n",
    "num_test_slots = len(test_indices)\n",
    "\n",
    "normalizer = 1.0 #np.sqrt(np.var(llr_train))\n",
    "llr_train = llr_train / normalizer\n",
    "llr_val = llr_val / normalizer\n",
    "\n",
    "# Reshape into samples x mod_order array.\n",
    "llr_train = llr_train.reshape(mod_order, -1).T\n",
    "llr_val = llr_val.reshape(mod_order, -1).T\n",
    "# Reshape into samples x 2 array.\n",
    "sym_train = sym_train.reshape(2, -1).T\n",
    "sym_val = sym_val.reshape(2, -1).T\n",
    "\n",
    "print(f\"Total number of slots in the training set: {num_train_slots}\")\n",
    "print(f\"Total number of slots in the validation set: {num_val_slots}\")\n",
    "print(f\"Total number of slots in the test set: {num_test_slots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a56988",
   "metadata": {},
   "source": [
    "## Model training and validation\n",
    "Model training is done using Keras here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2203dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[loss])\n",
    "model.fit(\n",
    "    x=sym_train,\n",
    "    y=llr_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(sym_val, llr_val),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb03acb",
   "metadata": {},
   "source": [
    "## Export to TensorRT\n",
    "Finally, the model gets exported to ONNX format. The ONNX format needs to be converted to TRT engine format to be consumed by the TensorRT inference engine, this is done here using the command line tool `trtexec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signature = [tf.TensorSpec([None, 2], tf.float32, name=\"input\")]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature)\n",
    "onnx.save(onnx_model, llrnet_onnx_file)\n",
    "print(\"ONNX model created. Converting to TRT engine file...\")\n",
    "command = f\"trtexec \" + \\\n",
    "    f\"--onnx={llrnet_onnx_file} \" + \\\n",
    "    f\"--saveEngine={llrnet_trt_file} \" + \\\n",
    "    f\"--skipInference \" + \\\n",
    "    f\"--minShapes=input:1x2 \" + \\\n",
    "    f\"--optShapes=input:42588x2 \" + \\\n",
    "    f\"--maxShapes=input:85176x2 \" + \\\n",
    "    f\"--inputIOFormats=fp32:chw \" + \\\n",
    "    f\"--outputIOFormats=fp32:chw\" + \\\n",
    "    f\"> /dev/null\"\n",
    "return_val = os.system(command)\n",
    "if return_val == 0:\n",
    "    print(\"TRT engine model created.\")\n",
    "else:\n",
    "    raise SystemExit(\"Failed to create the TRT engine file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4517945",
   "metadata": {},
   "source": [
    "## Define a PUSCH receiver chain using pyAerial\n",
    "\n",
    "This class encapsulates the whole PUSCH receiver chain. The components include channel estimation, noise and interference estimation, channel equalization and soft demapping, LDPC (de)rate matching and LDPC decoding. The receiver outputs the received transport block in bytes.\n",
    "\n",
    "The soft demapping part can be replaced by LLRNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Receiver:\n",
    "    \"\"\"PUSCH receiver class.\n",
    "\n",
    "    This class encapsulates the whole PUSCH receiver chain built using\n",
    "    pyAerial components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 llrnet_model_file,\n",
    "                 num_rx_ant,\n",
    "                 enable_pusch_tdi,\n",
    "                 eq_coeff_algo):\n",
    "        \"\"\"Initialize the PUSCH receiver.\"\"\"\n",
    "        self.cuda_stream = get_cuda_stream()\n",
    "\n",
    "        # Build the components of the receiver.\n",
    "        self.channel_estimator = ChannelEstimator(\n",
    "            num_rx_ant=num_rx_ant,\n",
    "            cuda_stream=self.cuda_stream\n",
    "        )\n",
    "        self.channel_equalizer = ChannelEqualizer(\n",
    "            num_rx_ant=num_rx_ant,\n",
    "            enable_pusch_tdi=enable_pusch_tdi,\n",
    "            eq_coeff_algo=eq_coeff_algo,\n",
    "            cuda_stream=self.cuda_stream\n",
    "        )\n",
    "        self.noise_intf_estimator = NoiseIntfEstimator(\n",
    "            num_rx_ant=num_rx_ant,\n",
    "            eq_coeff_algo=eq_coeff_algo,\n",
    "            cuda_stream=self.cuda_stream\n",
    "        )\n",
    "        self.demapper = Demapper(mod_order=mod_order)\n",
    "        self.trt_engine = TrtEngine(\n",
    "            trt_model_file=llrnet_model_file,\n",
    "            max_batch_size=85176,\n",
    "            input_tensors=[TrtTensorPrms('input', (2,), np.float32)],\n",
    "            output_tensors=[TrtTensorPrms('dense_1', (8,), np.float32)],\n",
    "            cuda_stream=self.cuda_stream\n",
    "        )\n",
    "\n",
    "        self.derate_match = LdpcDeRateMatch(\n",
    "            enable_scrambling=True,\n",
    "            cuda_stream=self.cuda_stream\n",
    "        )\n",
    "        self.decoder = LdpcDecoder(cuda_stream=self.cuda_stream)\n",
    "        self.crc_checker = CrcChecker(cuda_stream=self.cuda_stream)\n",
    "        self.llr_method = \"llrnet\"\n",
    "\n",
    "\n",
    "    def set_llr_method(self, method):\n",
    "        \"\"\"Set the used LLR computation method.\n",
    "\n",
    "        Args:\n",
    "            method (str): Either \"aerial\" meaning the conventional log-likelihood\n",
    "                ratio computation, or \"llrnet\" for using LLRNet instead.\n",
    "        \"\"\"\n",
    "        if method not in [\"aerial\", \"logmap\", \"llrnet\"]:\n",
    "            raise ValueError(\"Invalid LLR computation method!\")\n",
    "        self.llr_method = method\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        rx_slot,\n",
    "        slot,\n",
    "        pusch_configs):\n",
    "        \"\"\"Run the receiver.\"\"\"\n",
    "        # Channel estimation.\n",
    "        ch_est = self.channel_estimator.estimate(\n",
    "            rx_slot=rx_slot,\n",
    "            slot=slot,\n",
    "            pusch_configs=pusch_configs\n",
    "        )\n",
    "\n",
    "        # Noise and interference estimation.\n",
    "        lw_inv, noise_var_pre_eq = self.noise_intf_estimator.estimate(\n",
    "            rx_slot=rx_slot,\n",
    "            channel_est=ch_est,\n",
    "            slot=slot,\n",
    "            pusch_configs=pusch_configs\n",
    "        )\n",
    "\n",
    "        # Channel equalization and soft demapping. Note that the cuPHY kernel actually computes both\n",
    "        # the equalized symbols and the LLRs.\n",
    "        llr, eq_sym = self.channel_equalizer.equalize(\n",
    "            rx_slot=rx_slot,\n",
    "            channel_est=ch_est,\n",
    "            lw_inv=lw_inv,\n",
    "            noise_var_pre_eq=noise_var_pre_eq,\n",
    "            pusch_configs=pusch_configs\n",
    "        )\n",
    "\n",
    "        # Use the LLRNet model here to get the log-likelihood ratios.\n",
    "        dmrs_syms = pusch_configs[0].dmrs_syms\n",
    "        start_sym = pusch_configs[0].start_sym\n",
    "        num_symbols = pusch_configs[0].num_symbols\n",
    "        num_prbs = pusch_configs[0].num_prbs\n",
    "        mod_order = pusch_configs[0].ue_configs[0].mod_order\n",
    "        layers = pusch_configs[0].ue_configs[0].layers\n",
    "        num_data_sym = (np.array(dmrs_syms[start_sym:start_sym + num_symbols]) == 0).sum()\n",
    "        if self.llr_method == \"llrnet\":\n",
    "            # Put the input in the right format.\n",
    "            eq_sym_input = np.stack((np.real(eq_sym[0]), np.imag(eq_sym[0]))).reshape(2, -1).T\n",
    "            # Run the model.\n",
    "            llr_output = self.trt_engine.run({\"input\": eq_sym_input})[\"dense_1\"]\n",
    "\n",
    "            # Reshape the output in the right format for the LDPC decoding process.\n",
    "            llr_output = np.array(llr_output)[..., :mod_order].T.reshape(mod_order, layers, num_prbs * 12, num_data_sym)\n",
    "            llr_output *= normalizer\n",
    "        elif self.llr_method == \"aerial\":\n",
    "            llr_output = llr[0]\n",
    "        elif self.llr_method == \"logmap\":\n",
    "            inv_noise_var_lin = self.channel_equalizer.ree_diag_inv[0]\n",
    "            inv_noise_var_lin = np.transpose(inv_noise_var_lin[..., 0], (1, 2, 0)).reshape(inv_noise_var_lin.shape[1], -1)\n",
    "            llr_output = self.demapper.demap(eq_sym[0], inv_noise_var_lin[..., None])\n",
    "\n",
    "        # De-rate matching and descrambling.\n",
    "        coded_blocks = self.derate_match.derate_match(\n",
    "            input_llrs=[llr_output],\n",
    "            pusch_configs=pusch_configs\n",
    "        )\n",
    "\n",
    "        # LDPC decoding of the derate matched blocks.\n",
    "        code_blocks = self.decoder.decode(\n",
    "            input_llrs=coded_blocks,\n",
    "            pusch_configs=pusch_configs\n",
    "        )\n",
    "\n",
    "        # Combine the code blocks into a transport block.\n",
    "        tb, _ = self.crc_checker.check_crc(\n",
    "            input_bits=code_blocks,\n",
    "            pusch_configs=pusch_configs\n",
    "        )\n",
    "\n",
    "        return tb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973dc60c",
   "metadata": {},
   "source": [
    "## Model testing on Aerial test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82068792",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mod_order == 2:\n",
    "    test_vector_filename = \"TVnr_7201_PUSCH_gNB_CUPHY_s0p0.h5\"\n",
    "elif mod_order == 4:\n",
    "    test_vector_filename = \"TVnr_7916_PUSCH_gNB_CUPHY_s0p0.h5\"\n",
    "elif mod_order == 6:\n",
    "    test_vector_filename = \"TVnr_7203_PUSCH_gNB_CUPHY_s0p0.h5\"\n",
    "filename = AERIAL_TEST_VECTOR_DIR + test_vector_filename\n",
    "input_file = h5.File(filename, \"r\")\n",
    "\n",
    "num_rx_ant = input_file[\"gnb_pars\"][\"nRx\"][0]\n",
    "enable_pusch_tdi = input_file[\"gnb_pars\"][\"TdiMode\"][0]\n",
    "eq_coeff_algo = input_file[\"gnb_pars\"][\"eqCoeffAlgoIdx\"][0]\n",
    "\n",
    "receiver = Receiver(\n",
    "    llrnet_trt_file,\n",
    "    num_rx_ant=num_rx_ant,\n",
    "    enable_pusch_tdi=enable_pusch_tdi,\n",
    "    eq_coeff_algo=eq_coeff_algo\n",
    ")\n",
    "\n",
    "# Extract the test vector data and parameters.\n",
    "rx_slot = np.array(input_file[\"DataRx\"])[\"re\"] + 1j * np.array(input_file[\"DataRx\"])[\"im\"]\n",
    "rx_slot = rx_slot.transpose(2, 1, 0)\n",
    "\n",
    "slot = np.array(input_file[\"gnb_pars\"][\"slotNumber\"])[0]\n",
    "\n",
    "# Wrap the parameters in a PuschConfig structure.\n",
    "pusch_ue_config = PuschUeConfig(\n",
    "    scid=input_file[\"tb_pars\"][\"nSCID\"][0],\n",
    "    layers=input_file[\"tb_pars\"][\"numLayers\"][0],\n",
    "    dmrs_ports=input_file[\"tb_pars\"][\"dmrsPortBmsk\"][0],\n",
    "    rnti=input_file[\"tb_pars\"][\"nRnti\"][0],\n",
    "    data_scid=input_file[\"tb_pars\"][\"dataScramId\"][0],\n",
    "    mcs_table=input_file[\"tb_pars\"][\"mcsTableIndex\"][0],\n",
    "    mcs_index=input_file[\"tb_pars\"][\"mcsIndex\"][0],\n",
    "    code_rate=input_file[\"tb_pars\"][\"targetCodeRate\"][0],\n",
    "    mod_order=input_file[\"tb_pars\"][\"qamModOrder\"][0],\n",
    "    tb_size=input_file[\"tb_pars\"][\"nTbByte\"][0],\n",
    "    rv=input_file[\"tb_pars\"][\"rv\"][0],\n",
    "    ndi=input_file[\"tb_pars\"][\"ndi\"][0]\n",
    ")\n",
    "# Note that this is a list. One UE group only in this case.\n",
    "pusch_configs = [PuschConfig(\n",
    "    ue_configs=[pusch_ue_config],\n",
    "    num_dmrs_cdm_grps_no_data=input_file[\"tb_pars\"][\"numDmrsCdmGrpsNoData\"][0],\n",
    "    dmrs_scrm_id=input_file[\"tb_pars\"][\"dmrsScramId\"][0],\n",
    "    start_prb=input_file[\"ueGrp_pars\"][\"startPrb\"][0],\n",
    "    num_prbs=input_file[\"ueGrp_pars\"][\"nPrb\"][0],\n",
    "    dmrs_syms=dmrs_fapi_to_bit_array(input_file[\"ueGrp_pars\"][\"dmrsSymLocBmsk\"][0]),\n",
    "    dmrs_max_len=input_file[\"tb_pars\"][\"dmrsMaxLength\"][0],\n",
    "    dmrs_add_ln_pos=input_file[\"tb_pars\"][\"dmrsAddlPosition\"][0],\n",
    "    start_sym=input_file[\"ueGrp_pars\"][\"StartSymbolIndex\"][0],\n",
    "    num_symbols=input_file[\"ueGrp_pars\"][\"NrOfSymbols\"][0]\n",
    ")]\n",
    "\n",
    "\n",
    "# Run the receiver with the test vector parameters.\n",
    "receiver.set_llr_method(\"llrnet\")\n",
    "tb = receiver.run(\n",
    "    rx_slot=rx_slot,\n",
    "    slot=slot,\n",
    "    pusch_configs=pusch_configs\n",
    ")\n",
    "\n",
    "# Check that the received TB matches with the transmitted one.\n",
    "tb_size = pusch_configs[0].ue_configs[0].tb_size\n",
    "if np.array_equal(np.array(input_file[\"tb_data\"])[:tb_size, 0], tb[:tb_size]):\n",
    "    print(\"CRC check passed!\")\n",
    "else:\n",
    "    print(\"CRC check failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68032c",
   "metadata": {},
   "source": [
    "## Model testing on synthetic/simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pusch_record in df.take(test_indices).itertuples(index=False):\n",
    "\n",
    "    user_data_filename = dataset_dir + pusch_record.user_data_filename\n",
    "    user_data = load_pickle(user_data_filename)\n",
    "    snr = user_data[\"snr\"]\n",
    "\n",
    "    rx_iq_data_filename = dataset_dir + pusch_record.rx_iq_data_filename\n",
    "    rx_slot = load_pickle(rx_iq_data_filename)\n",
    "\n",
    "    ref_tb = pusch_record.macPdu\n",
    "    tb_size = len(pusch_record.macPdu)\n",
    "    slot = pusch_record.Slot\n",
    "\n",
    "    # Wrap the parameters in a PuschConfig structure.\n",
    "    pusch_ue_config = PuschUeConfig(\n",
    "        scid=pusch_record.SCID,\n",
    "        layers=pusch_record.nrOfLayers,\n",
    "        dmrs_ports=pusch_record.dmrsPorts,\n",
    "        rnti=pusch_record.RNTI,\n",
    "        data_scid=pusch_record.dataScramblingId,\n",
    "        mcs_table=pusch_record.mcsTable,\n",
    "        mcs_index=pusch_record.mcsIndex,\n",
    "        code_rate=pusch_record.targetCodeRate,\n",
    "        mod_order=pusch_record.qamModOrder,\n",
    "        tb_size=tb_size\n",
    "    )\n",
    "    # Note that this is a list. One UE group only in this case.\n",
    "    pusch_configs = [PuschConfig(\n",
    "        ue_configs=[pusch_ue_config],\n",
    "        num_dmrs_cdm_grps_no_data=pusch_record.numDmrsCdmGrpsNoData,\n",
    "        dmrs_scrm_id=pusch_record.ulDmrsScramblingId,\n",
    "        start_prb=pusch_record.rbStart,\n",
    "        num_prbs=pusch_record.rbSize,\n",
    "        dmrs_syms=dmrs_fapi_to_bit_array(pusch_record.ulDmrsSymbPos),\n",
    "        dmrs_max_len=1,\n",
    "        dmrs_add_ln_pos=1,\n",
    "        start_sym=pusch_record.StartSymbolIndex,\n",
    "        num_symbols=pusch_record.NrOfSymbols\n",
    "    )]\n",
    "\n",
    "    for llr_method in [\"aerial\", \"llrnet\", \"logmap\"]:\n",
    "\n",
    "        if snr not in tb_errors[llr_method].keys():\n",
    "            tb_errors[llr_method][snr] = 0\n",
    "            tb_count[llr_method][snr] = 0\n",
    "\n",
    "        receiver.set_llr_method(llr_method)\n",
    "        tb = receiver.run(\n",
    "            rx_slot=rx_slot,\n",
    "            slot=slot,\n",
    "            pusch_configs=pusch_configs\n",
    "        )\n",
    "\n",
    "        tb_count[llr_method][snr] += 1\n",
    "        tb_errors[llr_method][snr] += (not np.array_equal(tb[:tb_size], ref_tb[:tb_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a76951",
   "metadata": {},
   "outputs": [],
   "source": [
    "esno_dbs = tb_count[\"aerial\"].keys()\n",
    "bler = dict(aerial=[], llrnet=[], logmap=[])\n",
    "for esno_db in esno_dbs:\n",
    "    bler[\"aerial\"].append(tb_errors[\"aerial\"][esno_db] / tb_count[\"aerial\"][esno_db])\n",
    "    bler[\"llrnet\"].append(tb_errors[\"llrnet\"][esno_db] / tb_count[\"llrnet\"][esno_db])\n",
    "    bler[\"logmap\"].append(tb_errors[\"logmap\"][esno_db] / tb_count[\"logmap\"][esno_db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0bb3f-9b32-4df7-b296-9ed4a64e192e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "esno_dbs = np.array(list(esno_dbs))\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.01, 1)\n",
    "plt.xlim(np.min(esno_dbs), np.max(esno_dbs))\n",
    "plt.title(\"BLER Performance vs. Es/No\")\n",
    "plt.ylabel(\"BLER\")\n",
    "plt.xlabel(\"Es/No [dB]\")\n",
    "plt.grid()\n",
    "plt.plot(esno_dbs, bler[\"aerial\"], marker=\"d\", linestyle=\"-\", color=\"blue\", markersize=8)\n",
    "plt.plot(esno_dbs, bler[\"llrnet\"], marker=\"s\", linestyle=\"-\", color=\"black\", markersize=8)\n",
    "plt.plot(esno_dbs, bler[\"logmap\"], marker=\"o\", linestyle=\"-\", color=\"red\", markersize=8)\n",
    "plt.legend([\"Aerial\", \"LLRNet\", \"Log-MAP\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
