{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7eba9dd",
   "metadata": {},
   "source": [
    "# LLRNet: Dataset generation\n",
    "The wireless ML design flow using Aerial is depicted in the figure below. \n",
    "\n",
    "![ML design flow](ml_design_flow.png \"ML design flow\")\n",
    "\n",
    "In this notebook, we take data generated in the [Using pyAerial for data generation by simulation](example_simulated_dataset.ipynb) example and generate a dataset for training LLRNet using pyAerial. **Note that the data is assumed to have been generated prior to running this notebook.**\n",
    "\n",
    "LLRNet, published in\n",
    "\n",
    "O. Shental, J. Hoydis, \"*'Machine LLRning': Learning to Softly Demodulate*\", https://arxiv.org/abs/1907.01512\n",
    "\n",
    "is a simple neural network model that takes equalizer outputs, i.e. the complex-valued equalized symbols, as its input and outputs the corresponding log-likelihood ratios (LLRs) for each bit. This model is used to demonstrate the whole ML design flow using Aerial, from capturing the data to deploying the model into 5G NR PUSCH receiver, replacing the conventional soft demapper in cuPHY. In this notebook a dataset is generated. We use pyAerial to call cuPHY functionality to get equalized symbols out for pre-captured/-simulated Rx data, as well as the corresponding log-likelihood ratios from a conventional soft demapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8beba",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce597979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import cuda.bindings.runtime as cudart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "from aerial.phy5g.algorithms import ChannelEstimator\n",
    "from aerial.phy5g.algorithms import NoiseIntfEstimator\n",
    "from aerial.phy5g.algorithms import ChannelEqualizer\n",
    "from aerial.phy5g.algorithms import Demapper\n",
    "from aerial.phy5g.ldpc import LdpcDeRateMatch\n",
    "from aerial.phy5g.ldpc import LdpcDecoder\n",
    "from aerial.phy5g.ldpc import CrcChecker\n",
    "from aerial.phy5g.config import PuschConfig\n",
    "from aerial.phy5g.config import PuschUeConfig\n",
    "from aerial.util.data import PuschRecord\n",
    "from aerial.util.data import load_pickle\n",
    "from aerial.util.data import save_pickle\n",
    "from aerial.util.fapi import dmrs_fapi_to_bit_array\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2995f9",
   "metadata": {},
   "source": [
    "## Load the source data\n",
    "\n",
    "The source data can be either real data collected from an over the air setup, or synthetic data generated by simulation.\n",
    "\n",
    "**Note:** This notebook uses data generated using this notebook: [Using pyAerial for data generation by simulation](example_simulated_dataset.ipynb), which needs to be run before this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the source data directory which is assumed to contain the source data.\n",
    "DATA_DIR = \"data/\"\n",
    "source_dataset_dir = DATA_DIR + \"example_simulated_dataset/QPSK/\"\n",
    "# This is the target dataset directory. It gets created if it does not exist.\n",
    "target_dataset_dir = DATA_DIR + \"example_llrnet_dataset/QPSK/\"\n",
    "os.makedirs(target_dataset_dir, exist_ok=True)\n",
    "\n",
    "# Load the main data file.\n",
    "try:\n",
    "    df = pd.read_parquet(source_dataset_dir + \"l2_metadata.parquet\", engine=\"pyarrow\")\n",
    "except FileNotFoundError:\n",
    "    display(Markdown(\"**Data not found - has example_simulated_dataset.ipynb been run?**\"))\n",
    "\n",
    "print(f\"Loaded {df.shape[0]} PUSCH records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916133f",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "Here, pyAerial is used to run channel estimation, noise/interference estimation and channel equalization to get the equalized symbols, corresponding to the LLRNet input, as well as the log-likelihood ratios, corresponding to the LLRNet target output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d64188",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_stream = cudart.cudaStreamCreate()[1]\n",
    "\n",
    "# Take modulation order from the first record. The assumption is that all\n",
    "# entries have the same modulation order here.\n",
    "mod_order = df.loc[0].qamModOrder\n",
    "# These hard-coded too.\n",
    "num_rx_ant = 2\n",
    "enable_pusch_tdi = 1\n",
    "eq_coeff_algo = 1\n",
    "\n",
    "# Create the PUSCH Rx components for extracting the equalized symbols and log-likelihood ratios.\n",
    "channel_estimator = ChannelEstimator(\n",
    "    num_rx_ant=num_rx_ant,\n",
    "    cuda_stream=cuda_stream\n",
    ")\n",
    "noise_intf_estimator = NoiseIntfEstimator(\n",
    "    num_rx_ant=num_rx_ant,\n",
    "    eq_coeff_algo=eq_coeff_algo,\n",
    "    cuda_stream=cuda_stream\n",
    ")\n",
    "channel_equalizer = ChannelEqualizer(\n",
    "    num_rx_ant=num_rx_ant,\n",
    "    eq_coeff_algo=eq_coeff_algo,\n",
    "    enable_pusch_tdi=enable_pusch_tdi,    \n",
    "    cuda_stream=cuda_stream)\n",
    "derate_match = LdpcDeRateMatch(enable_scrambling=True, cuda_stream=cuda_stream)\n",
    "demapper = Demapper(mod_order=mod_order)\n",
    "decoder = LdpcDecoder(cuda_stream=cuda_stream)\n",
    "crc_checker = CrcChecker(cuda_stream=cuda_stream)\n",
    "\n",
    "# Loop through the PUSCH records and create new ones.\n",
    "pusch_records = []\n",
    "tb_errors = []\n",
    "snrs = []\n",
    "for pusch_record in (pbar := tqdm(df.itertuples(index=False), total=df.shape[0])):\n",
    "       \n",
    "    pbar.set_description(\"Running cuPHY to get equalized symbols and log-likelihood ratios...\")\n",
    "        \n",
    "    tbs = len(pusch_record.macPdu)   \n",
    "    ref_tb = pusch_record.macPdu\n",
    "    slot = pusch_record.Slot\n",
    "    \n",
    "    # Just making sure the hard-coded value is correct.\n",
    "    assert mod_order == pusch_record.qamModOrder\n",
    "\n",
    "    # Wrap the parameters in a PuschConfig structure.\n",
    "    pusch_ue_config = PuschUeConfig(\n",
    "        scid=pusch_record.SCID,\n",
    "        layers=pusch_record.nrOfLayers,\n",
    "        dmrs_ports=pusch_record.dmrsPorts,\n",
    "        rnti=pusch_record.RNTI,\n",
    "        data_scid=pusch_record.dataScramblingId,\n",
    "        mcs_table=pusch_record.mcsTable,\n",
    "        mcs_index=pusch_record.mcsIndex,\n",
    "        code_rate=pusch_record.targetCodeRate,\n",
    "        mod_order=pusch_record.qamModOrder,\n",
    "        tb_size=len(pusch_record.macPdu)\n",
    "    )\n",
    "    # Note that this is a list. One UE group only in this case.\n",
    "    pusch_configs = [PuschConfig(\n",
    "        ue_configs=[pusch_ue_config],\n",
    "        num_dmrs_cdm_grps_no_data=pusch_record.numDmrsCdmGrpsNoData,\n",
    "        dmrs_scrm_id=pusch_record.ulDmrsScramblingId,\n",
    "        start_prb=pusch_record.rbStart,\n",
    "        num_prbs=pusch_record.rbSize,\n",
    "        dmrs_syms=dmrs_fapi_to_bit_array(pusch_record.ulDmrsSymbPos),\n",
    "        dmrs_max_len=1,\n",
    "        dmrs_add_ln_pos=1,\n",
    "        start_sym=pusch_record.StartSymbolIndex,\n",
    "        num_symbols=pusch_record.NrOfSymbols\n",
    "    )]\n",
    "    \n",
    "    # Load received IQ samples.\n",
    "    rx_iq_data_filename = source_dataset_dir + pusch_record.rx_iq_data_filename\n",
    "    rx_slot = load_pickle(rx_iq_data_filename)\n",
    "    num_rx_ant = rx_slot.shape[2]\n",
    "\n",
    "    # Load user data.\n",
    "    user_data_filename = source_dataset_dir + pusch_record.user_data_filename\n",
    "    user_data = load_pickle(user_data_filename)\n",
    "    \n",
    "    # Run the channel estimation (cuPHY).\n",
    "    ch_est = channel_estimator.estimate(\n",
    "        rx_slot=rx_slot,\n",
    "        slot=slot,\n",
    "        pusch_configs=pusch_configs\n",
    "    )\n",
    "   \n",
    "    # Run noise/interference estimation (cuPHY), needed for equalization.\n",
    "    lw_inv, noise_var_pre_eq = noise_intf_estimator.estimate(\n",
    "        rx_slot=rx_slot,\n",
    "        channel_est=ch_est,\n",
    "        slot=slot,\n",
    "        pusch_configs=pusch_configs\n",
    "    )\n",
    "\n",
    "    # Run equalization and mapping to log-likelihood ratios.\n",
    "    llrs, equalized_sym = channel_equalizer.equalize(\n",
    "        rx_slot=rx_slot,\n",
    "        channel_est=ch_est,\n",
    "        lw_inv=lw_inv,\n",
    "        noise_var_pre_eq=noise_var_pre_eq,\n",
    "        pusch_configs=pusch_configs\n",
    "    )\n",
    "    ree_diag_inv = channel_equalizer.ree_diag_inv[0]\n",
    "    ree_diag_inv = np.transpose(ree_diag_inv[..., 0], (1, 2, 0)).reshape(ree_diag_inv.shape[1], -1)\n",
    "    \n",
    "    # Just pick one (first) symbol from each PUSCH record for the LLRNet dataset.\n",
    "    # This is simply to reduce the size of the dataset - training LLRNet does not\n",
    "    # require a lot of data.\n",
    "    user_data[\"llrs\"] = llrs[0][:mod_order, 0, :, 0]\n",
    "    user_data[\"eq_syms\"] = equalized_sym[0][0, :, 0]\n",
    "    map_llrs = demapper.demap(equalized_sym[0][0, :, 0], ree_diag_inv[0, ...])\n",
    "    user_data[\"map_llrs\"] = map_llrs\n",
    "\n",
    "    # Save pickle files for the target dataset.\n",
    "    rx_iq_data_fullpath = target_dataset_dir + pusch_record.rx_iq_data_filename\n",
    "    user_data_fullpath = target_dataset_dir + pusch_record.user_data_filename    \n",
    "    save_pickle(data=rx_slot, filename=rx_iq_data_fullpath)\n",
    "    save_pickle(data=user_data, filename=user_data_fullpath)\n",
    "    \n",
    "    pusch_records.append(pusch_record)\n",
    "    \n",
    "    #######################################################################################\n",
    "    # Run through the rest of the receiver pipeline to verify that this was legit LLR data.\n",
    "    \n",
    "    # De-rate matching and descrambling.\n",
    "    coded_blocks = derate_match.derate_match(\n",
    "        input_llrs=llrs,\n",
    "        pusch_configs=pusch_configs\n",
    "    )\n",
    "\n",
    "    # LDPC decoding of the derate matched blocks.\n",
    "    code_blocks = decoder.decode(\n",
    "        input_llrs=coded_blocks,\n",
    "        pusch_configs=pusch_configs        \n",
    "    )\n",
    "\n",
    "    # Combine the code blocks into a transport block.\n",
    "    tb, _ = crc_checker.check_crc(\n",
    "        input_bits=code_blocks,\n",
    "        pusch_configs=pusch_configs        \n",
    "    )\n",
    "    \n",
    "    tb_errors.append(not np.array_equal(tb[:tbs], ref_tb[:tbs]))\n",
    "    snrs.append(user_data[\"snr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28213ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving...\")\n",
    "df_filename = os.path.join(target_dataset_dir, \"l2_metadata.parquet\")\n",
    "df = pd.DataFrame.from_records(pusch_records, columns=PuschRecord._fields)\n",
    "df.to_parquet(df_filename, engine=\"pyarrow\")      \n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
