{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1665d4fe-b2ac-4d8e-8bff-8c9d762ccdb1",
   "metadata": {},
   "source": [
    "# Channel Estimation for Uplink Shared Channel (PUSCH) in PyAerial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f54fec-b1fd-4bba-851b-f62128141639",
   "metadata": {},
   "source": [
    "This notebook provides researchers an example of how to prototype machine learning in PyAerial. PyAerial is the Python bindings for the Aerial SDK, NVIDIA's L1 accelerated stack that is also integrated in the Aerial Omniverse Digital Twin (AODT). This enables researchers to develop standard-compliant approaches focusing on enhancing their link-level performance. Subsequently, the approach can be evaluated realistically in AODT, showing how the link-level performance translates to system-level KPIs.\n",
    "\n",
    "In particular, this notebook focuses on improving the channel estimation based on the DMRS pilots in a PUSCH transmission. First, we isolate the channel estimator block from the PyAerial PUSCH pipeline. The channel estimation is on one of the first receiver blocks, as seen in the figure below:\n",
    "\n",
    "![stack](stack_edited.png)\n",
    "\n",
    "To isolate the channel estimation block, we refer to the modular PUSCH pipeline in [Example PUSCH Simulation.ipynb](example_pusch_simulation.ipynb). There, we see how to interface channel estimation downstream with resource element (RE) demapper and with the wireless channel, and upstream with other components like the MIMO Equalizer. Similar approaches can be done for other blocks in the receiver or transmitter pipelines. \n",
    "\n",
    "**This notebook uses PyTorch to train a convolutional neural network that improves and interpolates least squares (LS) channel estimates. The training uses a custom LS estimator which interfaces with a resource grid, resource mapper, and channel generator from Sionna. The trained models are then integrated and validated in PyAerial PUSCH pipeline with standard-compliant signal transmission and reception blocks running on top of Sionna channels.**\n",
    "\n",
    "*<center>See below how we train and test the channel estimator models.</center>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a00c2d-d64b-4568-9477-74fcd9aa0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "\n",
    "# GPU setup\n",
    "GPU_IDX = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_IDX)  # Select only one GPU\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"  # Silence TensorFlow.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Our imports\n",
    "from channel_est_models import FusedChannelEstimator, ComplexMSELoss\n",
    "from channel_gen_funcs import (SionnaChannelGenerator, \n",
    "                               PyAerialChannelEstimateGenerator, \n",
    "                               sionna_to_pyaerial_shape)\n",
    "import utils as ut\n",
    "\n",
    "dev = torch.device(f'cuda')\n",
    "torch.set_default_device(dev)\n",
    "\n",
    "# General parameters\n",
    "num_prbs = 48               # Number of PRBs in the UE allocated bandwidth \n",
    "interp = 2                  # Interpolation factor = comb_size (2 or 4) = 2 for DMRS\n",
    "models_folder = f'saved_models_prbs={num_prbs}_interp={interp}' # Folder to save trained models\n",
    "\n",
    "# Training parameters\n",
    "train_snrs = np.arange(-10, 40.1, 10) # Train models for these SNRs.\n",
    "training_ch_model = 'UMa'   # Channel model ['Rayleigh', 'CDL-x', 'TDL-x', 'UMa', 'UMi'], \n",
    "                            # where x is in [\"A\", \"B\", \"C\", \"D\", \"E\"] as per TR 38.901\n",
    "n_iter = 500                # Number of training iterations. For best results: >20k\n",
    "batch_size = 32             # Batch size = number of channels to train simultaneously\n",
    "\n",
    "# Testing parameters\n",
    "test_snrs = np.arange(-10, 40.1, 5) # Test models for these SNRs.\n",
    "testing_ch_model = 'UMi'    # Channel for testing\n",
    "n_iter_test = 500           # Number of testing iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30f256-e687-4568-bd13-c067d7181e71",
   "metadata": {},
   "source": [
    "## Training channel estimation model\n",
    "\n",
    "The example machine learning model uses the least squares (LS) estimates and outputs a more accurate channel estimate. In its base configuration, the DMRS has 1/2 density in frequency (i.e. one RE for every two subcarriers). Our `ChannelEstimator`, therefore, needs to output twice many estimates as DMRS pilots to cover all subcarriers.\n",
    "\n",
    "*Important note on training*: Our approach consists of training one model per SNR. SNR-specific models can learn more accurately how to estimate the channels for SNRs close to the original SNR that was used for model training. This approach also solves the problem where low SNR channels incur in higher loss and lead to the model focusing on them and not working for the high SNR cases. \n",
    "\n",
    "For training, the model interfaces directly with Sionna channel models. For testing, the model is integrated in PyAerial's PUSCH pipeline and evaluated alongside other classic channel estimators, like the minimum mean squared error (MMSE) and the multi-stage MMSE (MS-MMSE). A diagram of training and testing is below:\n",
    "\n",
    "![train_sionna_test_pyaerial](train_test.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb0bd2-70d5-4b3f-9961-2f27f678f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = ut.get_model_training_dir(models_folder, training_ch_model, \n",
    "                                       num_prbs, n_iter, batch_size)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Channel generator for training\n",
    "train_ch_gen = SionnaChannelGenerator(num_prbs, training_ch_model, batch_size) \n",
    "\n",
    "n_sub = num_prbs * 12 // interp # number of subcarriers with reference symbols\n",
    "\n",
    "for snr_idx, snr in enumerate(train_snrs):\n",
    "    print(f'Training model for SNRs: {snr} dB')\n",
    "    save_model_path = ut.get_snr_model_path(models_dir, snr)\n",
    "    \n",
    "    model = FusedChannelEstimator(n_sub, comb_size=interp).to(dev)\n",
    "\n",
    "    criterion = ComplexMSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, mse_loss = [], []\n",
    "    count = []\n",
    "    for i in (pbar := tqdm(range(n_iter))): # trick: n_iter*(snr_idx+1), high SNR needs longer\n",
    "        # Sionna generate Channels\n",
    "        h, h_ls = train_ch_gen.gen_channel_jit(snr)\n",
    "\n",
    "        # Reshape to match exactly PyAerial's shapes \n",
    "        h_p    = sionna_to_pyaerial_shape(h.numpy(), n_sub, interp, est_type='mmse')\n",
    "        h_ls_p = sionna_to_pyaerial_shape(h_ls[..., ::interp].numpy(), n_sub, interp, est_type='ls')\n",
    "        \n",
    "        # Transition tensors to PyTorch\n",
    "        h_t, h_ls_t = torch.tensor(h_p).to(dev), torch.tensor(h_ls_p).to(dev)\n",
    "\n",
    "        inputs = torch.view_as_real(h_ls_t)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        h_hat = torch.view_as_complex(outputs)\n",
    "\n",
    "        loss = criterion(h_hat, h_t)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        \n",
    "        train_loss += [ut.db(loss.item())]\n",
    "        pbar.set_description(f\"Iteration {i+1}/{n_iter}\")\n",
    "        pbar.set_postfix_str(f\"Training loss: {train_loss[-1]:.1f} dB\")\n",
    "\n",
    "    last_model = save_model_path\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    ut.plot_losses([train_loss], ['train loss'], title=f'SNR = {snr} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1d9d1-56c1-4050-967a-8dcc06f6a5b1",
   "metadata": {},
   "source": [
    "## Testing channel estimation model\n",
    "The model trained above is a convolutional network, made of RESNET layers. This network consists of two separate blocks, each estimating as many subcarriers as reference signals. The subcarriers are then interleaved to compose the complete channel estimate. The diagram for this network is presented below for an example user allocated 48 PRBs:\n",
    "\n",
    "\n",
    "![model fused](model_fused.png)\n",
    "\n",
    "\n",
    "Now we evaluate this network using the LS channel estimates extracted from a PUSCH receiver, as opposed to manually extracted from the channel. The output of the network is compared with MS-MMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c40bd1-ed3f-40f3-be5e-d6df7326f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = ut.get_model_training_dir(models_folder, training_ch_model, \n",
    "                                      num_prbs, n_iter, batch_size)\n",
    "\n",
    "snr_losses_ls = []    # LS from PyAerial\n",
    "snr_losses_mmse = []  # MMSE from PyAerial \n",
    "snr_losses_mmse2 = [] # MMSE from PyAerial (median)\n",
    "snr_losses_ml = []    # ML channel estimation losses\n",
    "snr_losses_ml2 = []   # ML channel estimation losses (median)\n",
    "\n",
    "# Channel generator for testing\n",
    "test_ch_gen  = SionnaChannelGenerator(num_prbs, testing_ch_model, batch_size=32)\n",
    "\n",
    "# Create PyAerial channel estimate generator by applying PyAerial components on Sionna Channels\n",
    "pyaerial_ch_est_gen = PyAerialChannelEstimateGenerator(test_ch_gen)\n",
    "\n",
    "for snr_idx, snr in enumerate(test_snrs):\n",
    "    print(f'Testing SNR {snr} dB')\n",
    "\n",
    "    # Select model trained on the SNR closest to the test SNR\n",
    "    snr_model_idx = np.argmin(abs(train_snrs - snr))\n",
    "    snr_model = train_snrs[snr_model_idx]\n",
    "    print(f'Testing model trained on SNR {snr_model}')\n",
    "    \n",
    "    # Load ML model\n",
    "    model = FusedChannelEstimator(n_sub, comb_size=interp).to(dev)\n",
    "    model.load_state_dict(torch.load(ut.get_snr_model_path(train_dir, snr_model)))\n",
    "    \n",
    "    criterion = ComplexMSELoss()\n",
    "\n",
    "    model.eval()\n",
    "    ls_loss, mmse_loss, ml_loss = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_iter_test), desc='Testing LS & MS-MMSE in PyAerial'):\n",
    "            # Internally generate channels, add noise, receive the DM-RS symbols and estimate the channel\n",
    "            ls, mmse, gt = pyaerial_ch_est_gen(snr)\n",
    "            ls = ls[:,::interp//2] # to support comb4\n",
    "\n",
    "            # Reshape to match exactly PyAerial's shapes \n",
    "            ls_p   = sionna_to_pyaerial_shape(ls, n_sub, interp, est_type='ls')\n",
    "            mmse_p = sionna_to_pyaerial_shape(mmse, n_sub, interp, est_type='mmse')\n",
    "            gt_p   = sionna_to_pyaerial_shape(gt, n_sub, interp, est_type='mmse')\n",
    "\n",
    "            # Evaluate PyAerial classic estimators\n",
    "            for b in range(len(ls)):\n",
    "                ls_loss += [ut.complex_mse_loss(ls[b], gt[b][::interp])]\n",
    "                mmse_loss += [ut.complex_mse_loss(mmse[b], gt[b])]\n",
    "\n",
    "            # Evaluate ML approach\n",
    "            h, h_ls = torch.tensor(gt_p).to(dev), torch.tensor(ls_p).to(dev)\n",
    "            inputs = torch.view_as_real(h_ls)\n",
    "            outputs = model(inputs)\n",
    "            h_hat = torch.view_as_complex(outputs)\n",
    "            ml_loss += [criterion(h_hat, h).item()]\n",
    "            \n",
    "            # # Uncomment to inspect channel estimates vs ground-truth\n",
    "            # ut.compare_ch_ests([ls[0,:], \n",
    "            #                     mmse[0,:],\n",
    "            #                     h_hat.detach().cpu().numpy()[0,0,0,:,0],\n",
    "            #                     gt[0,:]], \n",
    "            #                    ['LS', 'MMSE', 'ML', 'GT'], title=f'SNR = {snr} dB')\n",
    "        \n",
    "    # Compute means and medians of LS, LS+ML and MS-MMSE\n",
    "    snr_losses_ml += [ut.db(np.mean(ml_loss))]\n",
    "    snr_losses_ml2 += [ut.db(np.median(ml_loss))]\n",
    "\n",
    "    snr_losses_ls += [ut.db(np.mean(ls_loss))]\n",
    "    snr_losses_mmse += [ut.db(np.mean(mmse_loss))]\n",
    "    snr_losses_mmse2 += [ut.db(np.median(mmse_loss))]\n",
    "\n",
    "    print(f'Avg. ML test loss for {snr} dB SNR is {snr_losses_ml[-1]:.1f} dB')\n",
    "\n",
    "    # Plot CDFs of MSE losses\n",
    "    ut.plot_annotaded_cdfs([ml_loss, mmse_loss], ['LS+ML', 'MS-MMSE'], \n",
    "                           title=f'MSE CDFs for SNR = {snr} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b256d-3fe5-4070-acc7-e7a4585fd1b9",
   "metadata": {},
   "source": [
    "\n",
    "**Observation**: When we fine tune our training, we see the ML model outperforming the MS-MMSE approach for most SNRs. The performance decays slightly when interpolation is necessary. Additionally, the ML seems more reliable for a wider class of channels. The variance of estimates is lower for ML, on average, it's performance saturates for high SNRs, even if the median continues to decay. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd69436-fe65-411a-91a4-470eec461807",
   "metadata": {},
   "source": [
    "## Plot comparison across SNRs\n",
    "**Requirement**: `test_snrs` must have more than one element. Read below why we would want to do this.\n",
    "\n",
    "**Model switching depending on SNR**: One of the challenges in channel estimation is having it work across SNRs. Lower SNRs have higher channel estimation mean squared error (MSE), which influences more heavily the loss of these samples in machine learning models, thus leading the model learn only low-SNR channels. One way to avoid this problem is to do a model-switching approach. In model-switching, each model is trained for a single SNR and use the model that has the closest SNR to the SNR of the user. \n",
    "\n",
    "Note that this approach requires a *sufficiently good* estimate of the SNR so the correct model is chosen. Usually, acquiring such an estimate is not difficult - for example, using the MMSE channel estimate should have more resolution than needed. As such, here we assume the SNR of the user is known and the closest model is selected. \n",
    "\n",
    "If we set `train_snrs = [-10, 0, 10, 20, 30, 40]` and `test_snrs = [-10, -5, 0, 5, 10, 15, ..., 40]`, then we will see that the model trained for an SNR of -10 dB is also used to estimate channels at -5 dB, and the model trained for 0 dB is also used at 5 dB, etc. This leads to a higher MSE in SNRs divisible by 5 but not 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4eeefd-5ff3-4e87-9eb6-bd7e44b98d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "\n",
    "plt.plot(test_snrs, snr_losses_ls,     '-', label='LS',             color='k', alpha=.7)\n",
    "plt.plot(test_snrs, snr_losses_ml,     '-', label='LS+ML (mean)',   color='tab:orange')\n",
    "plt.plot(test_snrs, snr_losses_ml2,   '--', label='LS+ML (median)', color='tab:orange')\n",
    "plt.plot(test_snrs, snr_losses_mmse,   '-', label='MMSE (mean)',    color='tab:green')\n",
    "plt.plot(test_snrs, snr_losses_mmse2, '--', label='MMSE (median)',  color='tab:green')\n",
    "\n",
    "plt.xlabel('SNR [dB]')\n",
    "plt.ylabel('NMSE [dB]')\n",
    "plt.xlim((min(test_snrs), max(test_snrs)))\n",
    "plt.legend(fontsize=7)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39bc022-0c92-41b3-996e-9fc4e3ee1511",
   "metadata": {},
   "source": [
    "Below is an example of this plot for the case `interp = 2`, trained with models every 5 dB SNRs, for 20k iterations, and 48 PRBs. \n",
    "\n",
    "![aerial_results](final_aerial_2x.png)\n",
    "\n",
    "Noteworthy ML gains in MSE compared to MS-MMSE median performance:\n",
    "\n",
    "- 4-7 dB gain for SNRs $\\in [-10, 0]$ dB\n",
    "- 3-4 dB gain for SNRs $\\in [ 0, 10]$ dB\n",
    "- 1-3 dB gain for SNRs $\\in [10, 20]$ dB\n",
    "\n",
    "Furthermore, when comparing mean performances (dashed lines), results indicate that the ML approach provides a more deterministic channel estimation, offering predictably lower errors also in high delay spread regimes. For channels at SNRs 20 dB, the benefit of ML is over 10 dB on average and it grows for higher SNRs. \n",
    "Note further that this approach is expected to work better for higher PRB allocations. Higher allocations allow the models to leverage more information across the band. However, performance should decrease when the interpolation factor (comb size) increases.\n",
    "\n",
    "### Considerations for Real Deployments\n",
    "\n",
    "For such approach to work in real deployments, it requires two additional steps we choose to omit here for simplicity: \n",
    "- SNR estimation: required to estimate the optimized model to perform channel estimation. Here, we consider the SNR is known and choose the closest model to that SNR. \n",
    "- PRB parallelization: during inference, the PRB parallelizer would split the LS estimates (e.g. 78 PRBs) into chunks that could be processed in parallel by the trained models of different sizes, and then put back together. As an example, if we trained models for {1, 4, 16} PRBs, the 78 PRB estimate would results in 4 batches for the 16 PRB model, 3 batches for the 4 PRB and 2 batches for the 1 PRB (4 * 16 + 3 * 4 + 2*1 = 64 + 12 + 2 = 78)\n",
    "\n",
    "### Assessing System-level Performance in the Aerial Omniverse Digital Twin\n",
    "\n",
    "This notebook can be used to generate models compatible with the machine learning example of PUSCH channel estimation in the AODT. As long as the `models_folder` variable is kept constant across runs, a single folder will be populated with the correct structure for multiple SNRs and PRBs. As mentioned in the AODT user guide, this folder will then need to be moved to a directory accessible by the AODT backend, and the `config_est.ini` file populated with the absolute path to the folder.\n",
    "\n",
    "**Benefits of using PyAerial as a bridge to AODT**\n",
    "\n",
    "- AODT uses a high-performance EM solver for computing raytracing propagation simulations. Raytracing is necessary for studying ML approaches in site-specific settings, offering insight and explainability to edge-cases previously unavailable in stochastic simulations.\n",
    "\n",
    "- AODT RAN simulations use the same software running on the same hardware deployed in the real world. This unprecedented combination creates an accurate system representation, giving researchers the possibility to design new features (AI/ML powered or not) and assess their network-wide end-to-end impact.\n",
    "\n",
    "- PyAerial currently provides a Python interface only to cuPHY, the PHY layer of Aerial. As such, comparions beyond the PHY are not possible in PyAerial, and the last link-level quantity that can be computed is block error rates. The AODT, on the other hand, integrates both Aerial's cuPHY and cuMAC, allowing researchers to measure how channel estimation impacts higher layers.\n",
    "\n",
    "For more information about how to run this ML channel estimation in AODT, see the [AODT user guide](https://docs.nvidia.com/aerial/aerial-dt/index.html).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
